---
title: "Longitudinal data & analysis"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(knitr)
require(ggplot2)
require(dplyr)
require(nlme)
require(lme4)
require(gridExtra)
library(sjstats)
```


### Introduction 

**Definition**: A longitudinal study refers to an investigation where participant outcomes and possibly treatments or exposures are collected at multiple follow-up times.

Repeated measurements and longitudinal data can be viewed as multilevel models:

* repeated observations (weightings - Level 1) are grouped 

* by individuals (children - Level 2) -- for each children, we have multiple weightings.


**Data example** Weight measuments of asian childern in the UK. Each child was weighted 1-5 times. 

| Variable    | Description         |
|-------------|---------------------|
| id          | identifier          |
| age         | age of the child    |
| weight      | weight of the child |
| sex         | gender              |





```{r}
child <- read.table("_dataW07/child.txt",header=TRUE)
str(child)
```





```{r}
ggplot(child, aes(age, weight, group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  theme_minimal()+
  labs(title = "Observed values")+
  theme(legend.position="none")
```

Looking at the figure, we can try to estimate

* Linear or quadratic trend
* Specific intercept for each child
* Specific regression line (slope) for each child

***

## Model with random intercept


$$\textit{weight}_{ij} = \beta_0 + \beta_1 \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij} + u_j + \varepsilon_{ij}$$
where we observe $i$-th weightings for $j=1, \dots, n$ individuals/children. 

### Syntax example: `lme()` from the `{nlme}` package

```{r}
child.mod1 <- lme(weight~age+I(age^2),random=~1|id,data=child)
summary(child.mod1)
```

* Note how variability (st. dev.) of $u_j$ and $\varepsilon_{it}$ are reported.  

* st. dev. of $u_j$ `(Intercept)` describes sq.root of variance among individuals.



### Syntax example: `lmer()` from the `{lme4}` package
```{r}
child.mod1a <-lmer(weight~age+I(age^2)+(1|id),data=child)
summary(child.mod1a)
# ICC
# Intercept Variance / (Intercept Variance + Residual Variance)
(ICC<- 0.8571/(0.8571+0.5478))
icc(child.mod1a)
```


```{r}
ggplot(child, aes(age, fitted(child.mod1), group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  theme_minimal()+
  labs(title = "Fitted values (random intercept)")+
  theme(legend.position="none")
```

***

## Model with random slopes

We observe greater variability among individuals than within individuals. 

We can allow for differences bewteen the global trajectory and the trajectories for each child.

Trajectory for each child would be represented by a separate line and the slope of that line varies from child to child.


$$\textit{weight}_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j}) \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij} + \varepsilon_{ij}$$
$$\textit{weight}_{ij} = \underbrace{\beta_0 + \beta_1 \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij}}_{fixed} + \underbrace{u_{0j} +u_{1j}\,\textit{age}_{ij} }_{random} + \varepsilon_{ij}$$




```{r}
(child.mod2<-lme(weight~age+I(age^2),random=~age|id,data=child))
# Extract "Regression table" for the fixed effects
summary(child.mod2)$tTable 
# lmer()  syntax example
(child.mod2a <-lmer(weight~age+I(age^2)+(1+age|id),data=child))
```

* We can see that the `Residual` (idiosyncratic) variability has decreased. 
* There is a positive correlation between the two random effects. 


----

#### We can test $H_0$ of the two random effects $u$ being uncorrelated: 

* Note that independence between $\varepsilon_{ij}$ and $u_j$ is assumed.

* We test if the correlation between $u_{0j}$ and $u_{1j}$ is equal to zero or not. 

* We do this by establishing a new model with uncorrelated errors (additional restriction imposed) & comparing the two models:

```{r}
(child.mod2a<-lme(weight~age+I(age^2),random = list(id=pdDiag(~age)),data=child))
summary(child.mod2a)$tTable # Estimated fixed effects do not differ much from "mod2"
anova(child.mod2a, child.mod2) # Compare the two models using F-test
```

The $p$-value shows that we do not reject the null hypothesis, i.e. correlation is assumed zero. 

--- 

#### Now, we can contrast if the slopes must be different: i.e. whether the random slope model provides significant improvement over the random intercept model.

Likelihood ratio test:   $LR = 2 (L_{ur}-L_r)$

```{r}
test = 2*(logLik(child.mod2a,REML=TRUE) - logLik(child.mod1, REML=TRUE))
mean(pchisq(test,df=c(0,1),lower.tail=FALSE)) # p-value of the chisq statistic
child.mod1.1 <- update(child.mod1,method="ML") # restricted
child.mod2a.1 <- update(child.mod2a,method="ML") # unrestricted
anova(child.mod2a.1,child.mod1.1)
```

The Likelihood ratio test suggests that we need a model with random slopes.


----

## Allowing for gender-based differences

```{r}
ggplot(child, aes(age, fitted(child.mod2), group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  theme_minimal()+
  labs(title = "Boys and girls combined")+
  theme(legend.position="none")
# Boys & girls separately, note the "facet_wrap()" layer
child$fitm2 <- fitted(child.mod2)
ggplot(child, aes(age, fitm2, group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  facet_wrap(~sex)+
  theme_minimal()+
  labs(title = "Boys and girls plotted separately")+
  theme(legend.position="none")
```


Figure shows a higher weight average for boys, and also a higher variability among boys that increases with their age.

A first step is to include the child sex and its interaction with age as a fixed effect in the model, i.e.:



$$\textit{weight}_{ij} = (\beta_0  + u_{0j}) + \delta_0 \textit{sex}_j + (\beta_1 + u_{1j}) \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij} +\beta_3 (\textit{sex}_j \times \textit{age}_{ij}) + \varepsilon_{ij}$$
$$\textit{weight}_{ij} = \underbrace{\beta_0 + \delta_0 \textit{sex}_j + \beta_1 \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij}+\beta_3 (\textit{sex}_j \times \textit{age}_{ij})}_{fixed} + \underbrace{u_{0j} +u_{1j}\,\textit{age}_{ij} }_{random} + \varepsilon_{ij}$$

```{r}
(child.mod3<-lme(weight~age+sex+age:sex+I(age^2),random=list(id=pdDiag(~age)),
                data=child))
summary(child.mod3)$tTable # note that "sex" is a binary factor
```

The variable sex is significant but the interaction is not.

Interpretation: On average, boys are heavier than girls but average rate of linear growth does not differ.

***


#### Heteroskedasticity

Until now, we have assumed mixed models with constant variance such that:

$$\mathbf{\textit{y}} = \mathbf{\textit{X }} \mathbf{\beta} + \mathbf{\textit{Z u}} +\mathbf{\varepsilon}     \,\,\,\,;\,\,\,\,  var(\mathbf{\varepsilon})=\sigma^2 \mathbf{\textit{I}}$$

This assumption does not hold in many situations:

* When the variance increases as long as the magnitude of the response
variable increases.
* When the variances are different for each group.
* When the variability depends on the explanatory variable.

In those situations, a better option is to model the variance as a functions of the covariates, a grouping factor or the mean of the response...




**Example: ** 

For a flexible FGLS-type approach where fitted values are used for weighting, we can use the `varPower()` variance function to model the heteroscedasticity:

```{r}
(child.mod5<-lme(weight~sex+age+I(age^2),
                random = list(id=pdDiag(~age)),
                data=child, weights=varPower(form=~fitted(.))))
summary(child.mod5)$tTable
```

For detailed discussion and additional heteroskedacity-robust estimators, see Dae-Jin Lee web page below).

This example was based on lecture by [Dae-Jin Lee](http://idaejin.github.io/bcam-courses/neiker-2016/material/mixed-models/) and slightly modified. [cc](https://creativecommons.org/licenses/by-sa/3.0/)