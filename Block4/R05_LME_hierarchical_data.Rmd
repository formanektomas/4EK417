---
title: "Hierarchical data and models"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(dplyr)
library(nlme)
library(lme4)
library(performance)
library(gridExtra)
library(lattice)
library(RLRsim)
```

## Hierarchical data (grouped, multilevel)

The dependent variable is measured once on each individual (unit of
interest) and individuals are grouped (nested) in more than one level.

The units of analysis are usually individuals (at a lower level) who
are nested within contextual/aggregate units (at a higher level).

Examples:

* student in schools.
* people in districts.
* patients in hospitals.
* plants in a plot.


Multilevel structures can be the result of experimental design. For example, consider a survey to study the health status: we can consider a design with three levels: 

* Level 1) we sample individuals  
* Level 2) individuals aggregated in districts  
* Level 3) districts aggregated to several regions  

---

## Empirical example - Data

[High School & Beyond](https://en.wikipedia.org/wiki/High_School_and_Beyond]) is a representative survey of U.S. public and Catholic high schools conducted by the National Center for Education Statistics (NCES). Data come from 160 schools, the average sample size per school is approximately 45 students.

* We are interested in the influence student's socio-economic status on math achievements

#### Data description:

| Variable | Description                                                          |
|----------|----------------------------------------------------------------------|
| School   |  a factor identifying the school that the studend attends            |
| Sex      |  Female or Male                                                      |
| Sector   |  Public or private                                                   |
| SES      |  a standardized scale of socio-economic status                       |
| MathAch  |  a measure of mathematics achievement                                |
| CSES     |  SES centered about the mean of the SES values for a corresponding school  |


```{r}
MathAchieve <- readRDS("data/MathAchieve.Rds")
str(MathAchieve)
```


* **Note:** This is how we can compute a new variable `CSES`, the centered individual `SES` using `{dplyr}` package. `CSES` is the relative standing on the `SES` measure for a student within school. We need this for simplified interpretation of parameters.
 
```{r}
MathAchieve %>% 
  group_by(School) %>% 
  mutate(CSES = floor(100*(SES-mean(SES)))/100) %>% # ?floor
  ungroup
```

---

## Empirical example - Model estimation


### We start by OLS approach:

* General notation:

$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$$
* `MathAch` model:

$$ \textit{MathAch}_i = \beta_0 + \beta_1 \textit{CSES}_i + \varepsilon_i$$ 

* We estimate $\textit{MathAch}$ as a function of relative socioeconomic status.  
* This model ignores that the students attend different schools.  
* Please note that index $i$ corresponds to individuals (students).


```{r}
summary(ml0 <- lm(MathAch~CSES, data=MathAchieve))
```


```{r, echo=FALSE}
plot(MathAchieve$CSES,MathAchieve$MathAch,cex=.5,col="grey")
abline(ml0,col=2,lwd=3)
```

---

##### Now, we want to evaluate differences between schools.

* Start with differences estimated as fixed effects

$$y_{ij} = \beta_{0j} + \beta_1 x_{ij} + \varepsilon_{ij}$$
where the index $j$ indicates the school of the student.

* With $\beta_{0j}$, we specify a separate intercept for each school.  
* What we include is a categorical variable with as many categories as schools.  
* The next model considers `School` as a fixed effect.


```{r}
(lm(MathAch ~ CSES + School-1, data = MathAchieve))
```

* You can see that the amount of $\beta_{0j}$ parameters estimated is overwhelming.  

We could generalize (i.e. complicate) the model by allowing for different `CSES` slope for each school (note the index $j$ for the slope $\beta_{1j}$):


$$y_{ij}=\beta_{0j} + \beta_{1j}x_{ij} + \varepsilon_{ij}$$

```{r}
Complex.mod <- lm(MathAch ~ School + CSES:School-1, data = MathAchieve)
# summary(Complex.mod)
# output not shown due to complexity and small relevance
```

* In most cases, we are not interested in these *particular school* effects, but in the population of schools -- in order to compare schools with different characteristics.

---- 

## LME - Multilevel model for group means

Hierarchical, nested examples:

* Students within schools (two levels).
* Students within classrooms within schools (three levels, hypothetical example, we don't have data for this).


##### Next Figure illustrates `MathAchieve` variability by schools and within schools (we took a subsample of schools with IDs `91`, `3`, etc.)

```{r}
grid.arrange(
  ggplot(MathAchieve[MathAchieve$School== c("91","3","31","52","74"),])+
      geom_boxplot(aes(School, MathAch)),
  ggplot(MathAchieve[MathAchieve$School== c("91","3","31","52","74"),])+
      geom_point(aes(School, MathAch)),
      ncol=2)
```


For hierarchical LME estimation, we specify two levels in the model:

* Level 1: $y_{ij} = \beta_{0j} + \varepsilon_{ij}$
    + where index $i$ corresponds to individuals and $j$ to schools.
    + For the simplest model, individual `MathAchieve` is given as School average + individual error.
    + If we consider Schools as a random effect, then $\beta_{0j}$ (the average of each school) would be given by:
  
* Level 2: $\beta_{0j} = \beta_0 + u_j$
    + where $\beta_{0j}$ is average of each school,
    + $\beta_0$ is total average of all schools (all students in all schools), 
    + $u_j$ is deviation of $j$-th school from total average. 
    
    
Now, we can write the previous model as: 

$$ y_{ij} = \beta_0 + u_j + \varepsilon_{ij}, \quad i=1,\dots,n_j \quad j=1,\dots,m$$

At both levels, we assume 

* $\varepsilon_{ij} \sim N(0,\sigma^{2}_{\varepsilon})$  

* $u_j\sim N(0,\sigma^2_{u})$ and 

* $\sigma^{2}_{\varepsilon}$ and $\sigma^2_{u}$ are independent.

The mean of $y$ for the $j$-th group is given by $\beta_0 + u_j$.

* Hence, $u_j$ is the deviation of the mean of $j$-th group from the total average $\beta_0$.

Individual level residuals $\varepsilon_{ij}$ are the difference between the value of the response variable of the $i$-th individual and the average of the group (School) they belong to. 


#### We fit the model with `lme`

* Model with random intercept, no regressors:

```{r}
ml1 <- lme(MathAch~1,random=~1|School, data=MathAchieve)
summary(ml1)
```



#### Extract beta coefficients (fixed effects)

```{r}
(beta0 <-  fixef(ml1))
```

##### Hence, $\hat{\beta}_{0j} = 12.637 + \hat{u}_j$

##### Residuals $\hat{u}_j$ may be extracted as: 

```{r}
fixef(ml1)-(predict(ml1, newdat=list(School=c("91","3","31","52","74"))))
```


##### Alternatively, we can plot of $\hat{\beta}_{0j}$ and confidence intervals (using `lmer()` package)

```{r}
ml2<-lmer(MathAch~1+(1|School),data=MathAchieve)
rr1 <- lme4::ranef(ml2, condVar = TRUE)
qqmath(rr1)
```

##### Repeat the same plot using `{ggplot2}`:


```{r, echo=T, eval=T}
# ggplot2 could be used as well:
rr1 <- data.frame(rr1) %>% mutate(sd1 = condval + condsd, sd2 = condval- condsd) 
ggplot(rr1, aes(as.numeric(grp),condval))+
    geom_ribbon(aes(ymin=sd2, ymax=sd1), fill = "lightgreen")+
    geom_point()+
    xlab("schools")+
    ylab("intercept")+
    coord_flip()
```

---

#### Quick note on variance

* If we had considered a fixed effect model (OLS) `y~1`, we would get a single variance $\hat{\sigma}^2$, obtained from `summary(lm(MathAch~1, ...))$sigma`, identical to the variance of $y$:

```{r}
summary(lm(MathAch~1, data=MathAchieve))$sigma^2
var(MathAchieve$MathAch)
```

* Considering a random effects model, `ml1 <- lme(MathAch~1,random=~1|School, ...)`, we have $\sigma^2_u$ and $\sigma^{2}_{\varepsilon}$. 

* Given model specification, total variability in the dependent variable can be expressed as:
$var(y_{ij}) = \sigma^{2}_{\varepsilon} + \sigma^2_u$ i.e. the variability within the schools plus the variability among the schools. 

```{r}
VarCorr(ml1)
```

* The difference between `var(MathAch)` and $\hat{\sigma}^2_{\varepsilon} + \hat{\sigma}^2_u$ is related to ML estimation method, see e.g. discussion [here](https://stats.stackexchange.com/questions/275620/why-is-the-estimated-variance-of-a-random-effect-not-equal-to-the-variance-of-it)

--- 

#### The intra-class correlation coefficient ($ICC$) is
$$\textit{ICC}=\frac{\sigma^2_u}{\sigma^2_u+\sigma^2_{\varepsilon}} = \frac{2,93^2}{2,93^2+6,26^2} \doteq 0,18$$

* "Correlation" of the outcome variable, measured between students from a given school, is equal to 0.18. 

----- 

### Testing random effect relevance


* Null hypothesis - random effects are superfluous, restricted model is sufficient 
    + $H_0: y_{ij} = \beta_0 + \varepsilon_{ij}.$

* Alternative hypothesis - LME model improves fit to the data
    + $H_1: y_{ij} = \beta_0 + u_j + \varepsilon_{ij}.$


```{r}
ml1a <-update(ml1, method="ML")
lm1 <- lm(MathAch~1,data=MathAchieve)
RLRsim::exactLRT(ml1a,lm1) # Restricted Likelihood Ratio Tests for additive and linear mixed models
```


------ 


## Hierarchical LME model with fixed slope


### CSES-regressor as fixed effect

* By choosing a common (fixed) slope, we have:

$$y_{ij}=  \underbrace{\beta_0 + \beta_1 x_{ij}}_{\textit{fixed effects}} + \underbrace{u_j }_{\textit{random effect}}+\varepsilon_{ij}$$

#### Model estimation:

```{r}
ml2 <- lme(MathAch ~ CSES, random = ~1|School, data=MathAchieve)
summary(ml2)
```

The fixed effects are

* $\hat{\beta_0} = 12.65$
* $\hat{\beta_1}  = 2.19$

$\beta_0$ is the average math achievement for those students with average socioeconomic status.

* Because `CSES` is a centered `SES`, `CSES` equals zero for individuals with average socioeconomic status.

The average straight line is given by:

$$\textit{MathAch} = 12.65 + 2.19 \cdot \textit{CSES}$$

#### Plot of fitted values:
(note the identical slopes)

```{r}
fits <- MathAchieve %>% select(CSES, School)
fits$fit <- fitted(ml2)
ggplot(fits)+
  geom_line(aes(CSES,fit, group=School, color=School), size=1.2)+ 
  theme_minimal()+
  theme(legend.position="none")
```


----------

#### Specification test:


Besides the $t$-test of statistical significance for `CSES`, we may  perform a Likelihood Ratio Test for the fixed effects by means of maximum likelihood (ML not REML): 

* Null hypothesis: fixed effects on `CSES` are superfluous for describing variability of dependent variable.  

```{r}
ml2a <-update(ml2, method="ML")
anova(ml1a,ml2a)
```

------ 

### CSES-regressor with random effect

* In this model, we assume that the relationship between the response variable and the explanatory variables could be different for each of the units at Level 2, i.e. the relationship may change from school to school. 

We generalize the previous model by allowing for a random slope for each group (school): 

* Level 1: $y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \varepsilon_{ij}$

* Level 2: $\beta_{0j} = \beta_0 + u_{0j}$
* $~~~~~~~~~~~\beta_{1j} = \beta_1 + u_{1j}$



Putting the equations together, we have:
 
 $$y_{ij}=  \underbrace{\beta_0 + \beta_1 x_{ij}}_{\textit{fixed effects}} + \underbrace{u_{0j} + u_{1j} x_{ij} }_{\textit{random effects}} + \varepsilon_{ij}$$
 LME model assumptions:
 
 $$ \bigg(\matrix{u_{0j} \\ u_{1j}}\bigg) \sim N(\mathbf{0},\mathbf{G})$$
 $$\mathbf{G} =  \bigg(\matrix{\sigma^2_{u_0} & \sigma_{u_0u_1} \\ \sigma_{u_1u_0} & \sigma^2_{u_1}}\bigg)$$

where $\sigma_{u_0u_1}$ is the covariance between the random intercepts and slopes. A positive value of the covariance implies that groups with high $u_0$ effect tend to have high values of the $u_1$ effect, or equivalently, school with high intercepts also have higher slopes.


#### Model estimation:

```{r}
ml3 <- lme(MathAch ~ CSES, random = ~CSES|School, data=MathAchieve)
summary(ml3)
```

* The slope is $+2.19$ and the variance of the slopes among schools is $0.833^2$. 
* On average, we predict an increase of $2.19$ units in the math achievement score when CSES increases one unit. 


#### ICC
```{r}
performance::icc(ml3)
```

* Differences between schools explain 19% of “remaining” variance – i.e. after the variance explained by fixed effects is subtracted.

---

#### Plot of fitted values:
(note the school-specific slopes)

```{r}
fits <- MathAchieve %>% select(CSES, School)
fits$fit <- fitted(ml3)
ggplot(fits)+
  geom_line(aes(CSES,fit, group=School, color=School), size=1.2)+ 
  theme_minimal()+
  theme(legend.position="none")
```


---

#### Testing model specification: random slope redundancy 


* We test if the random effect on slope (generalization of the model) is justified (last model is tested against fixed slopes):

```{r}
ml3a <-update(ml3, method="ML")
anova(ml2a,ml3a)
```


-----

#### Testing model specification: independence of $u_0$ and $u_1$ random effects


* $\sigma_{u_0u_1}$ (the covariance between the intercepts of the groups and the slopes) is an estimated parameter. 

* In order to simplify our model and its interpretation, we may want to test $H_0: \sigma_{u_0u_1} = 0$ against  $H_1: \sigma_{u_0u_1} \neq 0$. 

* Under the null hypothesis, we can write: 

$$y_{ij}=  \underbrace{\beta_0 + \beta_1 x_{ij}}_{\textit{fixed effects}} + \underbrace{u_{0j} + u_{1j} x_{ij} }_{\textit{random effects}} + \varepsilon_{ij},$$

 
 $$ \bigg(\matrix{u_{0j} \\ u_{1j}}\bigg) \sim N(\mathbf{0},\mathbf{G}),$$
 $$\mathbf{G} =  \bigg(\matrix{\sigma^2_{u_0} & 0 \\ 0 & \sigma^2_{u_1}}\bigg).$$



* To perform the test, we estimate a restricted model (note the `pdDiag()` argument) and compare the results with the unrestricted model:

```{r}
ml3b <- lme(MathAch~CSES,random = list(School=pdDiag(~CSES)),
data=MathAchieve, method="ML")
summary(ml3b)
# Evaluate the restriction using LR test
anova(ml3a,ml3b)
```

* We do not reject $H_0$.

-----

### CSES as random effect + Sector as fixed effect at Level 2

* `Sector` has only two levels (factor: Public/Private)  
* We can add a new variable: `Sector`. It varies only at the school level. Also, we can use `Sector:CSES` interaction (Sector-specific slope of the CSES variable).

* Level 1: $y_{ij} ~= \beta_{0j} + \beta_{1j}x_{ij} + \varepsilon_{ij}$

* Level 2: $\beta_{0j} = \beta_0 + \beta_2 s_j + u_{0j}$
* $~~~~~~~~~~~\beta_{1j} = \beta_1 + \beta_3 s_j + u_{1j}$

Again, this can be combined into:

 $$y_{ij}=  \underbrace{\beta_0 + \beta_1 x_{ij} +\beta_2 s_j + \beta_3 (x_{ij} \times s_j)}_{\textit{fixed effects}} + \underbrace{u_{0j} + u_{1j} x_{ij}}_{\textit{random effects}}+ \varepsilon_{ij}$$

#### Model estimation

```{r}
ml4 = lme(MathAch~CSES+Sector+CSES*Sector,random = list(School=pdDiag(~CSES)),
          data=MathAchieve)
summary(ml4)
```

All the fixed effects in `ml4` are significant. 

Additional test for relevance of inclusion of variable `Sector` may be performed as follows


```{r}
ml4a <- update(ml4,method="ML")
anova(ml3b,ml4a)
```


#### Plot fitted data

```{r}
fits2 <- MathAchieve %>% select(CSES, School, Sector)
fits2$fit <- fitted(ml4)
ggplot(fits2)+
  geom_line(aes(CSES,fit, group=School, color=Sector),alpha=0.7, size=1.2)+
  theme_minimal()
```

Private schools have higher average: $+2.79$ and lower slope (with a difference in slope equal to $-1.34$) than Public schools. 

--- 

