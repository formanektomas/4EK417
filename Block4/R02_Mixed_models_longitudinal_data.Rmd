---
title: "Longitudinal data & analysis"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(dplyr)
library(performance)
library(nlme)
library(lme4)
library(gridExtra)
library(sjstats)
```


### Longitudinal data: Introduction 

**Definition**: A longitudinal study refers to an investigation where participant outcomes and possibly treatments or exposures are collected at multiple follow-up times.

Repeated measurements and longitudinal data can be viewed as multilevel models:

* repeated observations (weightings - Level 1) are grouped 

* by individuals (children - Level 2) -- for each children, we have multiple weightings.


**Data example** 

Weight measuments of asian childern in the UK. Each child was weighted 1-5 times. 


| Variable    | Description         |
|-------------|---------------------|
| id          | identifier          |
| age         | age of the child    |
| weight      | weight of the child |
| sex         | gender              |





```{r}
child <- read.table("data/child.txt",header=TRUE)
str(child)
```





```{r}
ggplot(child, aes(age, weight, group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  theme_minimal()+
  labs(title = "Observed values")+
  theme(legend.position="none")
```

Looking at the figure, we can try to estimate

* Linear or quadratic trend
* Specific intercept for each child
* Specific regression line (slope) for each child

---

## LME - model with random intercept


$$\textit{weight}_{ij} = \beta_0 + \beta_1 \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij} + u_j + \varepsilon_{ij},$$
where we observe $i$-th weightings for $j=1, \dots, n$ individuals/children. 

#### Syntax example: `lme()` from the `{nlme}` package

```{r}
child.mod1 <- lme(weight~age+I(age^2),random=~1|id,data=child)
summary(child.mod1)
```

* Note how variance (st. dev.) of $u_j$ and $\varepsilon_{it}$ are reported.  

* st. dev. of $u_j$ `(Intercept)` describes sq.root of variance among individuals.

* In our example, RE are child-specific. RE variance (st. dev.) describes variability associated with differences among children.


#### Syntax example: `lmer()` from the `{lme4}` package
```{r}
child.mod1a <-lmer(weight~age+I(age^2)+(1|id),data=child)
summary(child.mod1a)
# ICC
# Intercept Variance / (Intercept Variance + Residual Variance)
(ICC<- 0.8571/(0.8571+0.5478))
# Differences between children explain ~61% of the variance that’s “left over” after the variance explained by our fixed effects.
#
performance::icc(child.mod1a)
```

--- 

#### Plot fitted values of the LME model with RE on intercept


```{r}
ggplot(child, aes(age, fitted(child.mod1), group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  theme_minimal()+
  labs(title = "Fitted values (LME with random intercept)")+
  theme(legend.position="none")
```

***

## Model with random slopes

* We can allow for differences between the global trajectory and the trajectories for each child.

* Trajectory for each child would be represented by a separate line and the slope of that line varies from child to child.


$$\textit{weight}_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j}) \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij} + \varepsilon_{ij}$$
$$\textit{weight}_{ij} = \underbrace{\beta_0 + \beta_1 \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij}}_{fixed} + \underbrace{u_{0j} +u_{1j}\,\textit{age}_{ij} }_{random} + \varepsilon_{ij}$$

#### LME estimation: RE on the intercept and `age`:


```{r}
#lme() syntax example
child.mod2<-lme(weight~age+I(age^2),random=~age|id,data=child)
summary(child.mod2) 
# lmer()  syntax example
#
child.mod2a <-lmer(weight~age+I(age^2)+(1+age|id),data=child)
summary(child.mod2a)
```

* We can see that the `Residual` (idiosyncratic) variance has decreased 
    + from `0.55` (random intercept only), 
    + to `0.33` (random intercept and slope). 

* There is a non-zero (positive) correlation between the two random effects. 


----

#### We can test null hypothesis of the two random effects $u$ being uncorrelated: 

* Note that independence between $\varepsilon_{ij}$ and $u_j$ is assumed.

* We test if the correlation between $u_{0j}$ and $u_{1j}$ is equal to zero or not. 

* Not rejecting the null hypothesis simplifies our model (one less coeffcient to estimate).  

* Test is performed by establishing a new model with uncorrelated errors (additional restriction imposed) & comparing the two models:

```{r}
child.mod2a<-lme(weight~age+I(age^2),random = list(id=pdDiag(~age)),data=child)
summary(child.mod2a) # Estimated fixed effects do not differ much from "mod2"
anova(child.mod2a, child.mod2) # Compare the two models using F-test
```

The $p$-value shows that we do not reject the null hypothesis, i.e. correlation is assumed zero. 

--- 

#### Now, we can contrast if the slopes must be different: i.e. whether the random slope model provides significant improvement over the random intercept model.

Likelihood ratio test:   $LR = 2 (L_{ur}-L_r)$

```{r}
# "Manual" LR test
test = 2*(logLik(child.mod2a,REML=F) - logLik(child.mod1, REML=F))
mean(pchisq(test,df=c(0,1),lower.tail=FALSE)) # p-value of the chisq statistic
# LR test though the anova() function
child.mod1.1 <- update(child.mod1,method="ML") # restricted
child.mod2a.1 <- update(child.mod2a,method="ML") # unrestricted
anova(child.mod2a.1,child.mod1.1)
```

* The Likelihood ratio test suggests that we need a model with random slopes.
    + With large sample sizes, p-values based on the likelihood ratio are generally considered okay.
    + With small samples, use the Kenward-Roger or Satterthwaite approximations (for REML models) from the `pbkrtest` package. 
   
   
**Note:** 

* `REML` stands for restricted maximum likelihood and it is the default estimator for LME models.  
* `ML` stands for maximum likelihood. However, `ML` estimates are known to be biased - they may underestimate variance of the random effects.  

* With `REML` being usually less biased, `REML` estimates of variance components are generally preferred. 
    + In previous models, we skipped setting estimation method and left it at default (`REML`).  

* `REML` assumes that the fixed effects structure is correct. You should always use `ML` when comparing models with different fixed effects. 
    + In our example, fixed effects are identical, so the `ML` syntax used for *LR* test is just illustrative. 

Even though we use `ML` to compare models, we should report parameter estimates from `REML`-estimated model, as `ML` may underestimate variance of the random effects.


----

## Allowing for gender-based differences

```{r}
ggplot(child, aes(age, fitted(child.mod2), group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  theme_minimal()+
  labs(title = "Boys and girls combined")+
  theme(legend.position="none")
# Boys & girls separately, note the "facet_wrap()" layer
child$fitm2 <- fitted(child.mod2)
ggplot(child, aes(age, fitm2, group=as.factor(id), color=as.factor(id)))+
  geom_line()+
  geom_point()+
  facet_wrap(~sex)+
  theme_minimal()+
  labs(title = "Boys and girls plotted separately")+
  theme(legend.position="none")
```


* Figure shows a higher weight average for boys, and also a higher variability among boys that increases with their age.

* Generally, we want **random effect to have at least five levels** (approximatelly). So, we fit sex (a two level factor: male or female) as a fixed, not random, effect.

* We include child's sex and its interaction with age as a fixed effect in the model, i.e.:



$$\textit{weight}_{ij} = (\beta_0  + u_{0j}) + \delta_0 \textit{sex}_j + (\beta_1 + u_{1j}) \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij} +\beta_3 (\textit{sex}_j \times \textit{age}_{ij}) + \varepsilon_{ij}$$
$$\textit{weight}_{ij} = \underbrace{\beta_0 + \delta_0 \textit{sex}_j + \beta_1 \textit{age}_{ij} + \beta_2 \textit{age}^2_{ij}+\beta_3 (\textit{sex}_j \times \textit{age}_{ij})}_{fixed} + \underbrace{u_{0j} +u_{1j}\,\textit{age}_{ij} }_{random} + \varepsilon_{ij}$$

```{r}
child.mod3<-lme(weight~age+sex+age:sex+I(age^2),random=list(id=pdDiag(~age)),
                data=child)
summary(child.mod3) # note that "sex" is a binary factor
```

The variable `sex` is significant but the interaction is not.

Interpretation: On average, boys are heavier than girls but average rate of linear growth does not differ.

---


#### Heteroscedasticity

Until now, we have assumed mixed models with constant variance such that:

$$\mathbf{\textit{y}} = \mathbf{\textit{X }} \mathbf{\beta} + \mathbf{\textit{Z u}} +\mathbf{\varepsilon}     \,\,\,\,;\,\,\,\,  var(\mathbf{\varepsilon})=\sigma^2 \mathbf{\textit{I}}$$

This assumption does not hold in many situations:

* When the variance increases as long as the magnitude of the response
variable increases. 
* When the variances are different for each group (each child in our example). 
* When the variability depends on magnitudes of explanatory variables. 

In those situations, we used heteroscedasticity-robust estimates: we model the variance as a functions of the covariates, a grouping factor or the mean of the response...




**Example: ** 

For a flexible FGLS-type approach where fitted values are used for weighting, we can use the `varPower()` variance function to model the heteroscedasticity:

```{r}
# Inspect residuals from a FGLS estimator:
(child.mod5<-lme(weight~sex+age+I(age^2)+age:sex,
                random = list(id=pdDiag(~age)),
                data=child, weights=varPower(form=~fitted(.))))
summary(child.mod5)$tTable
plot(child.mod5, resid(., type="p") ~ fitted(.), abline=0)
```

* For a detailed discussion on assumptions in LME models, see e.g. [https://ademos.people.uic.edu/Chapter18.html](https://ademos.people.uic.edu/Chapter18.html)



