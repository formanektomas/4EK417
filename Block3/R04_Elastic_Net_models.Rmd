---
title: "Ridge/lasso/elastic net regression"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(111)
library(knitr)
library(plotly)
library(ggplot2)
library(ISLR)
library(glmnet)
```

## Data 

```{r}
Hitters <- Hitters
str(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters) # here, complete cases analysis is performed 
# NA values are in the dependent variable column (Salary) only.
```
## Model/task description

+ Major League Baseball Data from the 1986 and 1987 seasons.  
+ We will use the data included in the Hitters dataset to predict players' **Salary** (our dependent variable).  

## Dataset description

| Variable   | Description                                                                         |   |
|------------|-------------------------------------------------------------------------------------|---|
| AtBat      |  Number of times at bat in 1986                                                     |   |
| Hits       |  Number of hits in 1986                                                             |   |
| HmRun      |  Number of home runs in 1986                                                        |   |
| Runs       |  Number of runs in 1986                                                             |   |
| RBI        |  Number of runs batted in in 1986                                                   |   |
| Walks      |  Number of walks in 1986                                                            |   |
| Years      |  Number of years in the major   leagues                                             |   |
| CAtBat     |  Number of times at bat during his   career                                         |   |
| CHits      |  Number of hits during his career                                                   |   |
| CHmRun     |  Number of home runs during his   career                                            |   |
| CRuns      |  Number of runs during his career                                                   |   |
| CRBI       |  Number of runs batted in during   his career                                       |   |
| Cwalks     |  Number of walks during his career                                                  |   |
| League     |  A factor with levels A and N   indicating player's league at the end of 1986       |   |
| Division   |  A factor with levels E and W   indicating player's division at the end of 1986     |   |
| PutOuts    |  Number of put outs in 1986                                                         |   |
| Assists    |  Number of assists in 1986                                                          |   |
| Errors     |  Number of errors in 1986                                                           |   |
| Salary     |  1987 annual salary on opening day   in thousands of dollars                        |   |
| NewLeague  |  A factor with levels A and N   indicating player`s league at the beginning of 1987 |   |


---


# Ridge regression


## Data preparation (for ridge, lasso and elastic net penalties):


`model.matrix()` is from the `{stats}` package, 

* Factors are expanded to a set of dummy variables.


`glmnet()` function from the `{glmnet}` package needs regression input in matrix $\mathbf{X}$ and vector $\mathbf{y}$ form.

```{r}
x <-  model.matrix(Salary~., Hitters)[ , -1] # 1st column with (Intercept) is removed
y <- Hitters$Salary
```


The function `glmnet()` performs the **elastic net** estimation.

* Regressor standardization is performed automatically by default (may be turned off).


**Elastic net** is based on a general formula that encompasses both lasso and ridge regression


* alpha = 0 for ridge regression
* alpha = 1 for lasso regression
* 0 < alpha < 1 for elastic net regression


For illustration, we estimate model for lambda < 0.001 , 10^7 > 


* Otherwise, lambda (regularization/penality) coefficient can be selected automatically 

```{r}
grid <- 10^seq(-3, 7, length=100) # grid/sequence of 100 lambda values
ridge.mod <- glmnet(x, y ,alpha=0, lambda=grid) # alpha=0 <--> ridge reg.
dim(coef(ridge.mod))
```

`ridge.mod` is a dataframe object that stores model coefficients (20 coefficients) for each of the 100 models estimated (for each lambda from the `grid` object, a model is estimated)

```{r}
plot(ridge.mod, xvar="lambda", label=TRUE)
```

Coefficient value plot as a function of log of lambda. `glmnet()` estimates multiple models on a grid of values of lambda.


When $\lambda$ (log of $\lambda$) is large, all the $\beta_j$ coefficients ($j \neq 0$) are essentially zero. Then as we relax the lambda restriction parameter, the coefficients grow away from zero in a smooth way, and the sum of squares of the coefficients is getting bigger and bigger until we reach a point where lambda is effectively zero, and the coefficients are unregularized (equal to OLS from `lm()`, up to rounding errors).

## Sample coefficients for different lambda values:

**High lambda**

```{r}
#value of lambda
ridge.mod$lambda[3]
log(ridge.mod$lambda[3])
coef(ridge.mod)[,3]
# Note that the intercept is not restricted and its estimated value
# is very close to the sample mean of the dependent variable
mean(Hitters$Salary)
```
**Lower lambda**
```{r}
#value of lambda
ridge.mod$lambda[80]
log(ridge.mod$lambda[80])
coef(ridge.mod)[,80]
```

## Model selection by cross-validation

* Ridge regression `glmnet()` gives a whole *path of models* and we need to pick one. 

* `{glmnet}` has a built-in function `CV.glmnet()` that will do cross validation:


```{r}
cv.ridge <- cv.glmnet(x,y,lambda=grid,alpha=0)
plot(cv.ridge)
```

With high lambda (right hand side of the plot), the mean squared error (*MSE*) is very high and the coefficients are restricted to be small. 


For lower lambda values, at some point, the *MSE* kind of levels off. The plot seems to indicate that the full model is doing a good job (we might go for the "full" OLS, instead of parameter shrinkage).

* Note the two two vertical lines. One is at the minimum *MSE* and the other vertical line corresponds to a more restricted model (higher $\lambda$) that is within one standard error of the minimum. 

* To select model at lambda with minimum MSE, use: `s=lambda.min` (coefficient extraction)

* For the largest value of $\lambda$ such that error is within 1 standard error of the minimum, use: `s=lambda.1se` (model is more restricted, hence less prediction variance)

* At the top of the plot, you can see how many non-zero variables coefficients are in the model. Here, there's all 19 variables in the model (19 variables plus the intercept) and no coefficient is zero (as we are doing ridge regression, regressors are not turned off).

## Coefficient extraction

There's a coefficient function extractor that works on a cross validation object and picks the coefficient vector corresponding to the best model:

 + Note: beta coefficients from ridge regression are not unbiased estimates of marginal effects (as in OLS)  

```{r}
coef(cv.ridge, s="lambda.min") # alternatively, use "lambda.1se"
cv.ridge$lambda.min
```

## Predictions
Sample fitted values for the first 5 observations in the Hitters dataset
```{r}
predict(cv.ridge, newx=x[1:5,], s="lambda.min")
predict(cv.ridge, newx=x[1:5,], s="lambda.1se")
Hitters[1:5,"Salary", drop=F] # for comparison
```


---


#  Lasso regression

## Model/task description

We perform the same task: will use the data included in the Hitters dataset to predict players' **Salary** (our dependent variable). 


However, lasso performs both


* shrinkage
* and variable selection.

## Plots

For illustration, we estimate the lasso model for lambda < 0.001 , 10^4 > 

```{r}
grid <- 10^seq(-3, 4, length=100)
lasso.mod <- glmnet(x, y ,alpha=1, lambda=grid)
dim(coef(lasso.mod))
```

Again, `lasso.mod` is a 100 x 20 dataframe.

```{r}
plot(lasso.mod, xvar="lambda", label=TRUE)
```

As lambda values increase, different coefficients are shrunken or turned off.



## Model selection by cross-validation

```{r}
cv.lasso <- cv.glmnet(x,y,lambda=grid,alpha=1)
# you can remove the `lambda=grid` argument and cv.glment() selects a restricted (reasonalbe) 
# lambda range
plot(cv.lasso)
```


## Coefficient extraction for `lambda.min` and `lambda.1se`:

```{r}
coef(cv.lasso, s="lambda.min")
coef(cv.lasso, s="lambda.1se") # s="lambda.1se" is the default setting
cv.lasso$lambda.min
cv.lasso$lambda.1se
```


## Predictions
Sample fitted values for the first 5 observations in the Hitters dataset
```{r}
predict(cv.lasso, newx=x[1:5,], s="lambda.min")
predict(cv.lasso, newx=x[1:5,], s="lambda.1se")
Hitters[1:5,"Salary", drop=F] # for comparison
```

---


#  Elastic net regression

## Model/task description

For this illustration, we set $\alpha = 0.5$.

## Plots

For illustration, we estimate the elastic net model for lambda < 0.001 , 10^4 > 

```{r}
grid <- 10^seq(-3, 4, length=100)
EN.mod <- glmnet(x, y ,alpha=0.5, lambda=grid)
```

Again, `EN.mod` is a 100 x 20 dataframe.

```{r}
plot(EN.mod, xvar="lambda", label=TRUE)
```


## Model selection by cross-validation

```{r}
cv.EN <- cv.glmnet(x,y,lambda=grid,alpha=0.5)
plot(cv.EN)
```


## Coefficient extraction for `lambda.min` and `lambda.1se`:

```{r}
coef(cv.EN, s="lambda.min")
coef(cv.EN, s="lambda.1se") # s="lambda.1se" is the default setting
cv.EN$lambda.min
cv.EN$lambda.1se
```


## Predictions
Sample fitted values for the first 5 observations in the Hitters dataset
```{r}
predict(cv.EN, newx=x[1:5,], s="lambda.min")
predict(cv.EN, newx=x[1:5,], s="lambda.1se")
Hitters[1:5,"Salary", drop=F] # for comparison
```